SPEAKER 0
this material is made available to you by or on behalf of the University of Melbourne under section 103 P of the Copyright Act 1968. It may be subject to copyright. For more information, visit the university copyright website.

SPEAKER 1
Hi, Simon. That's better.

SPEAKER 2
Health health is more than just you and me.

SPEAKER 1
Uh, OK, if there isn't.

SPEAKER 2
Yeah, there is. And I don't know what we what we say we

SPEAKER 1
can have a yarn.

SPEAKER 2
So were you in the office today?

SPEAKER 1
I'm in my office now. I had a lecture lecture earlier today at 2. 15. So, um, I had to be here. Yeah. I had really wanted to come in today, but I

SPEAKER 2
We had plumbing challenges since this morning. So the the life of a homeowner, that's true.

SPEAKER 1
Uh, I think I'll be next on Wednesday. Yeah. Fang's giving her confirmation talk. So, like, 1150 right? Yeah, something like that. I'll definitely be there. And then, uh, i'll be in on Thursday, I think. Yep. You have a digital health group meeting Thursday? I suppose so.

SPEAKER 2
But we had one person for for for a minute. I've I I've not been logging in early, because then it just leaves us, you know, 10 minute dead time on the video of people might just be used to,

SPEAKER 1
um what else was I gonna say? Uh, something. Oh, yeah. Um, that PhD shouldn't you mentioned like, um, like, I just unfortunately just have no more slots left, I think to take on another.

SPEAKER 2
All right, well, we've already beat last semester. Let's see. And I know that one of the students is joining from a distant time zone, so it's, like, 4. 30 in the morning. So that's that's devotion. Well, Simon, let's wait a couple of minutes and then, um, can you go ahead and share your screen, though I don't have anything to OK, well, uh, welcome to lecture. 11 in the winter intensive of digital transformation of health. Uh, we're tonight. We're joined by Doctor Simon de Alonso, who's my colleague in computing and information systems and co taught this subject with me last year. Um, but this year, we we just get this one, a lecture from him. I am home alone with three dogs who I've tried to bribe with pig's ears. Um, so hopefully they will leave me alone, Um, for a while, But, um are there? Let's see. Uh, I any like burning questions that I can address before we turn over to Simon? Ok, well, Simon, um, small class, but we do have some of our really, uh um engaged people here, so I think that if we want to, uh, get some conversation going there'll there'll, there'll be some conversant, uh, here and I'll, I'll turn it over to you.

SPEAKER 1
Thanks. I think if people have questions, they should just say something or put up a virtual hand. We have a small class, we can make this informal and we can make a conversational So I do digital mental health stuff. And hence my presentation of this lecture topics for today will be the following. It'll look at digital mental health apps slash interventions as a starting point before moving on to my particular areas of interest regarding artificial intelligence and mental health topics such as have what's called digital phenotyping using N LP for analysing text to get like mental health insights. Also the use of chat bots as digital mental health tools or interventions. And finally, I'm going to end by looking at some of the ethical and human computer interaction considerations in this space, we first ask why Digital mental health and I can just give you some of the sort of general talking points and stats here. Um, it's sort of not really news by now that mental and substance use disorders, um a. A leading cause of disability worldwide. It's a significant, uh, health issue. 24 people in the world will be affected by mental health or neurological issues at some point in their lives, according to some World Health Organisation statistics. And almost half of Australians experience mental ill health in their lifetime. And 54% of people with mental illness do not access any treatment. I kind of am mindful of the terminology, I suppose, used in this context or thrown around here. Um, sometimes I do feel that there's a tendency of some, like maybe the mental health industry to sometimes maybe catastrophize things and use terms like disorder maybe too often or mental illness. There is a broad spectrum ranging from like serious mental illnesses, you know, lighter general mental malaise and, uh, what I like to say, just broad term mental ill health, so it doesn't, you know. I mean, it's necessarily a major mental illness, but even just cases of optimising mental health promoting positive psychology, positive well-being and mental health would be thrown under these umbrellas. Um, as I see it. So with regards to mental, um, health issues, they are worsened by delayed treatment due to serious problems in detection and accurate diagnosis. The proportion of people with mental illness accessing treatment is half that of people with physical disorders. So when I'm using illness here, I can perhaps, you know, consider things like major depressive disorder General anxiety disorder. Um, under those terms, many affected by mental ill health do not get the treatment they need due to stigma living in regional areas or because of low income. And there's a substantial labour cost of mental, um, health ill illness in, uh, Australia, apparently estimated to be over 12 billion annually. This would be in time in terms of like people taking sick leaves. They, you know, extra sickies during their work here and other things. So why apps? What can they do? Mental health apps can help to overcome these impediments to treatment and fill in resource gaps. Uh, since the development of the first established mental health app around 2010, around 10,000 mental health and well-being apps have been introduced to the market. It sounds like an awful lot. That's just because, you know, anyone can whip up an app. Whether it's good or not is another question, and this is a big issue. These apps generally serve to complement or supplement, rather than completely replace traditional forms of face to face treatment. Um, and types of interventions we can consider the first is teletherapy, which is pretty straightforward. In a sense, it's just about doing person to person psychotherapy over online chat or video conference. Um, and like I said, that's pretty straightforward. It's being used all the time now as, um, a way to, um, implement talk therapy. Especially like during COVID-19 times when you know, um, face to face interactions were, um, either sort of very difficult or impossible. During our lockdown periods, Teletherapy really came into its own. We also have this term blended interventions, which is about using an app that is integrated with traditional clinical mental health care. So, like the APP is used by clinicians or practitioners in concert with their clients. And it all sort of melds together to basically use the mental health platform as one piece of the puzzle of the treatment puzzle. And then you have the option also of stand alone or fully automated mental health apps, where, like the mental health APP itself, is stand alone and constitutes intervention. It doesn't necessarily need any human, um, helper in the picture. Somebody could just, like, go online, get, uh, download some automated stand on mental health app and just use it one on one, the client and the APP directly, um, as I was alluding to before, there's an issue with APP quality and the need to provide app evaluation frameworks or approaches around. Pardon only around 1%. Or maybe 23, if we're lucky, have been professionally evaluated as there are currently known. Four standards, um, and or regulations, um, standards and regulations. Um, so standards and regulation frameworks are, however developing and here here are just some examples of some bodies that do these type of thing, such as the, uh, Australian TG A has a regulation of software as a medical device framework, which is emerging and the US Food and Drug Administration. The FDA has, um, a digital Therapeutics framework. Um, and an important point to make here is to definitely note the difference between apps that employ evidence based approaches versus apps and apps that themselves have an evidential basis. So to illustrate or to make that clearer, uh, some, if not all of you have heard of, say, one of the most evidence based and common psychotherapy techniques, which is cognitive behavioural therapy, has a solid evidence base, and an app might just go. Oh, we you know, Inc we we've incorporated C BT as it's called into our app. But that's different to actually incorporating C BT into the app and then doing like a trial of the APP Does the app itself work? Um, in terms of its incorporation of C BT. Uh, picking the most suitable app is difficult because of privacy risks, usability concerns. And because apps constantly update and change, users typically rely on the use of star rating systems and reviews in APP stores despite evidence that these evaluation methods can be misleading. So if you go into the Google play store, if or if you do like an app search on Google. You may very well see some type of star rating system or just like those short user pithy, um, short user reviews. They might be something, but there's probably gonna be a need for a more rigorous or reliable uh means to kind of also have access to information that evaluates apps. So, as I've just sort of said, there's a need for solid evaluation frameworks moving away from picking the best rated app and towards making an informed decision on the basis of clinically relevant criteria. Say, you know, apps maybe are somewhat different to music or Netflix recommendations, which might be, you know, it's might might be good enough sometimes just to see what the most um, popular or hot movies of the day are. Um, here are just two examples where you have APP evaluation frameworks one's called One Mind Cyber Guide. I was actually there today for another reason. So let's just click on that so I just provide these links in these slides for you to also use yourself. Brian will have or will have upload loaded the slides, these slides to the L MS. Um, for this week so, yeah. Trying to see if there's a example of an app, so I can Quickly. No, I think. Mm. Let's just quickly look at this one, too. This is from I think, the American Psychiatric Association. Oh, no. They no longer have these. That's a shame. I'm curious. Have I moved to somewhere else?

SPEAKER 2
Diamond? I'll search for that. While you you can keep, uh, please search for it.

SPEAKER 1
Uh, the A P. A app evaluation model. Well, I actually just talked about the A P. A app evaluation model here. I'll just quickly go over some of the steps. Um, the first step in evaluating is to gather background information. Such as? What is the business model? If the APP is free, then how does it support its own development? Who is the developer? Does he claim to be medical costs? What platforms does it work on? Uh, and things like, when was it last updated? Step two Are considerations concerning risk, privacy and security Like, is there a privacy policy? What? Data collected? Can you opt out of data collection? Can you delete the data? All those types of things. Um, this one does it support? Uh P. Sorry. Does it support H IP a compliance? Um, What does? Hi. A stand for Brian. You know that acronym?

SPEAKER 2
Uh, health information portability.

SPEAKER 1
Something like that. It's an American body. Or so the American standards for, like, preserving, um, the, uh, integrity, like security and that privacy integrity of health data. Uh, next step is that of evidence. What does it claim to do? Worse, versus what does it actually do? Is there peer reviewed published evidence about the tool or science behind it? Um, feedback, Uh, does the content appear of at least reasonable value That might be a bit trickier to gauge, but at least from a prima facie consumer perspective, this is an interesting one here. I find that a lot of offerings, there'll be a few papers about them, but like the papers are driven by the people who run the company and that it might be like an academic who creates a start up and then, like the all of the free papers that are written by the APP, have been led by that academic co-founder, for example. So it's another consideration. Um, does like the app have independent evaluations from people who don't have a direct interest in the APP itself. Ease of use. Is it easy to access for the patient at hand? Would it be easy to use on a long term basis? Is it customizable? Um, can it be, you know, active without Internet connection or not? Um, for those who need cultural relevance, you know, um, from diverse cultures, they might need to consider that or people who need, um, accommodation in terms of, um, in, like, impaired vision or other disabilities. Inter operative ability. Things like, who owns the data? Can it share data with your, uh, your local electronic health record? Um, and can it, like, work with other devices or other systems? Um, can you, like, look at the data yourself or download it yourself or print it? So they're the five steps that this a P, a app evaluation model suggest That's another link. Brian, did you were you able to find a more active link?

SPEAKER 2
I did. Um, yeah, I sent it on the show. The chat II. I think it's the same thing that you were looking at a question.

SPEAKER 1
Yes. Oh, go ahead and say what you're gonna say. I had nothing to say. I'll just put your chat links, OK?

SPEAKER 2
So I'm just gonna ask you kind of like my current, uh, pet peeve about like, like, status quo versus, like, new things. Um, the the evaluation all seems good, but when, you know, I kind of wondered. Ok, so there's evidence that, like, cognitive behavioural therapy works. Um And so then we say, Well, we don't know whether cognitive behavioural therapy works with this app, but we also don't know how good my particular therapist is at implementing, uh, C BT. So I'm just kind of curious your thoughts about, uh, this kind of status quo bias and is there? Uh um I mean, I think it kind of an asymmetry there that's necessary or unnecessary.

SPEAKER 1
Uh, I think that's an interesting point. How would I dismiss your point? Um, I would say that in the terms of an, uh, like a real therapist, you're right. Like some people do C BT better than others or some people are just generally better therapists than others. And there are things like, you know, therapeutic rapport, which are essential. My feeling would be that well, in the case of a therapist um, they can at least say I have training and they can do what they can do. If a if a client is not getting what they want from a therapist, they can easily just move on, I suppose. And I would say also that, um when therapists are training, they are evaluated for their ability, their fidelity, their technique and stuff. And there are even once they become practitioners, there are ways to evaluate them. They can get, like, evaluated by their peers or colleagues periodically. Or there are like session rating scales where, uh, uh, a client can rate their therapist after a session, for example, so they can be. There are avenues to accountability for the human practitioner in terms of their ability to, you know, do something and they're adaptable, whereas an app, you know, it's There's only one app. If it's gonna be claiming something, it has to back that up. That would be my answer. Hi, Claire. Hello.

SPEAKER 3
Um, yeah. Hi. I just had a question about, like, this app evaluation model. Is that who's supposed to use that? Is that for the users themselves to evaluate it? Or for um, a clinician who's recommending the app, or is it a framework for developing it?

SPEAKER 1
Uh, my answer would be both, Like both cons, Uh, not consumer, both client and clinician. Um, and I suppose it would behove the developers to also, you know, um, familiarise yourself with this framework because I suppose if they can, um, accommodate these points in their own app, they would get a better evaluation score themselves. So everyone has a vested interest, I suppose. Yeah. Think, um, So just for those who are interested, you know, if you want to do your own extracurricular stuff, give us some more on standards and principles for evaluating mental health apps, a couple of papers there. And I like this. This last quote from the second paper, Um, just as no single best antidepressant or therapy exists, no single best app exists to treat all patients or mental illnesses. Clinical presentation, patient preferences, technology, literacy, accessibility and treatment goals are all important factors that determine the best course of care for any patient. And such factors must be considered when deciding what the most suitable app might be for a patient in any given situation. Which reminds me I was having a chat with somebody today talking about recommender systems and how, um you know there's room to develop recommender systems for not just one single app, like, you know, to deliver content within an app. But I recommend a system for, like, a Google app store or something in terms of mental health apps. For example, if there are a few 1000 offerings out there, perhaps there could be systems whereby an individual puts in a variety of information about themselves, and then the most suitable recommendation is given to them, using some, you know, recommend recommended system techniques. Uh, so what are the examples of mental health conditions with an associated mental health act? Well, basically, a lot of them are the two most common mental ill health conditions associated with mental health a absolute anxiety and depression. That's simply because they're the two most common conditions amongst the population. One in three people going to doctors in Australia state that they have either or both of these conditions are beyond these two. Their apps can be found for psychosis especially, you know, psychosis is a more specific and, um um involved kind of, um, condition, and you'll really only find work on that being done within clinical institutes largely, Um, or if there is sort of some industry work on it, it probably has connections, uh, to clinical institutes, Because, yeah, you need that. Um, bipolar disorder is another serious mental illness, um, post traumatic stress disorder, eating disorders and others. And, um, yeah, that's this is what I was just saying about psychosis slash schizophrenia. Um, so these apps generally work in support of therapy and or medication. As I've alluded to earlier, the National Institute of Mental Health classifies mental health apps into six categories based on functionality. These are self management apps, cognition, improvement, skills training, social support, symptom tracking and passive data collection. Um, I mean, you can have more than one of these in an app, I suppose, and I'll be looking at this in more detail Come the second part of this lecture. Um, and they span all stages of the clinical care provision, including immediate crisis intervention, prevention, diagnosis, primary treatment, supplementary in-person therapy, and posttreatment condition management. Um, mobile apps are a good choice for psychological treatment. Delivered delivery compared to other platforms due to ease of habit, low effort expectancy and high hedonic motivation. I'll touch upon these more as we go. But essentially, apps are just like such a Swiss Army knife of digital devices. And, um, you know, they can both do detection and they can do delivery in that like you can do sensing with them as I'll talk more about later. People are always using them. They're ubiquitous. They're economical, et cetera. Um, just one more link there for those who are interested, do mental health, uh, do mental health, mobile apps, work evidence and recommendations for designing high efficacy mental health mobile apps. Um, because this is one of the things they're out there. Work's been done on them, but they're still, you know, uh, maybe it's not that early days anymore, but it's still not, you know, settled. And there still needs to be work done on, I suppose, working out how to improve efficacy, determining efficacy possibilities, um, promoting app engagement, you know, like a lot of apps suffer from dropout rates because just people don't, you know, get enough out of them or just get bored with them too soon. And all these other issues, I suppose, in a sense, ultimately the jury is still out on whether these things, um, you know, as some type of Well, they're not. They're not a silver bullet, but, you know, are they good enough? Um, many mental health apps are tailored to the individual through questionnaires that determine the users needs strengths and weaknesses. So we're getting towards this idea of, like, personalization, which I'll be talking more about. And that's provided to of activities for the user to for the user involving things like self help, skills, identification of and support for therapeutic goals. Uh, promotion of positive self talk. Reinforcement of techniques used in therapy. C BT. As I was saying then, other modalities such as behavioural activation, mindfulness positive psychology, positive psychology is just basically about promoting, like, positive aspects of mental states rather than just treating psychology as needing to eliminate negative mental states. Um, other things like, uh, you know, meditations, relaxation and breathing exercises. That's a big one. The market for mental, um, mindfulness and meditation apps. You, Some of you probably already kind of aware of that, um on level one of Melbourne Connect, where I am at the university, we have a Pifer contemplative studies you know, and they they kind of just study, um, meditation practises for well-being, and that, and one of their interests is looking at, like, mental health apps for these things. Oh, not mental health apps. I should just say well-being apps that are based on meditations and relaxation and reflection. Um, I'll just skip the rest. And that too. So that's a bit about mental health apps. I also wanna, uh, mention even, uh, a little, um, about virtual reality, because that's kind of emerging more and more so. VR is an immersive, computer generated simulation of a 3D environment or experience that can be interacted with using a handset or other input devices. A lot of you have probably heard about VR, but I just provide that definition for completeness there. And VR can be used as a therapeutic tool for mental health issues such as anxiety disorders, phobias and post traumatic stress disorder. Um mm. I would say for phobias, this is probably where it started because it sort of stands to reason. Or, you know, it kind of makes sense that phobias would be something that could be, um, tackled using VR because, um, obviously people with phobias have an aversion to like, the real, you know, something that exists. So at least as a starting point, if you can kind of trigger their their their senses, you know, with the visualisation of that thing. But they know it's not the actual real thing. Then it can obviously serve as a stepping stone. Um, for the individual to become more comfortable with the object of their phobia, how it can be used for more conceptual or kind of, Um hm. Mind based for want of a better term mental health issues is an interesting question. Uh, perhaps just by, like, helping uses, they, um, visualise certain things the objects of their, uh, mental health issue, Uh, in order to be able to be to deal with them better, like if they could, um, you know, CBT is like, basically about de learning how to deal with negative thoughts and that so if those negative thoughts could be visually represented somehow in the VR simulation and then the person could engage with them, like in a game format and, like, you know, quash them or something, they're therefore eliminating their negative thoughts in some type of gamified way VR way similar to PTSD What I just said, uh, VR therapy allows individuals to safely confront and overcome fears and traumas in a controlled environment without being exposed to real life triggers. That basically just sums up what I've been saying could also be used for relaxation and stress reproduction through immersive experiences such as nature walks and mindfulness exercises. So another possibility in terms of its outcomes, showing promising results in research studies, with some studies reporting similar or even better outcomes. Compared to traditional therapy methods, however, it's still new and needs more research. Some challenges in VR include the cost of equipment and software, as well as potential side effects such as motion sickness and disorientation. But to be honest, only a minority of people would probably be afflicted by those things. Um, despite these challenges, V I has the potential to be a valuable tool. Yeah, so that wraps up the kind of app slash intervention portion of, um, this like show. And what's the time? 6. 42. We still got a bit of time. Now I'm gonna talk about a i mental health. So with the advent of digital approaches to mental health. Modern A I machine learning in particular, is being used in the development of prediction, detection and treatment solutions for mental health care. I'd say that although there has been considerable progress in digital health and the application of a I to physical health in general, the adoption of a I mental health is relatively nascent. Um, topics include digital phenotyping, natural language process analytics and chatbots. And we're gonna look at these one by one. So, yeah, uh, for various reasons, um, you know, fields like oncology, radiology, bioinformatics, the kind of a they've adopted a I, I suppose, or advanced computational methodologies for a while now, um, in various ways. In the mental health domain, it's exciting, but it hasn't caught on as much for for various reasons. One is perhaps, um, there are less straightforward use case possibilities in the mental health domain compared to those other domains. Um, I think another one is that you know, the people who work in radiology and that they're already pretty kind of advanced. When it comes to medical practitioners who use technologies. There's like pe, the mental health practitioners, the psychologist with a pen and paper and a couch trying to kind of introduce a I into their world might be a little less obvious for them. OK, and what would it even mean? Like chat bots that somehow semi replace them Or, you know, using NLP to assess what they're doing. It's a little more complicated and also, you know, the complexity of the human mind and hetero heterogeneous nature of mental health concerns. Um, much more varied in a way than, like, you know, what can happen to a hamstring on me or something like that, This sort of pontificating there or thinking about, you know, possible reasons. Uh, digital phenotyping or DP. For short, our increasing usage of smartphones wearables on the Internet, particularly social media, network and networking slash networking, has increased what can be termed our digital footprint or data exhaust. I prefer the term digital footprint, probably, and digital phenotyping is about mining slash analysing of individuals or groups, for that matter digital footprint to gain insight into their mental health, insights of clinical value that could be used to anticipate mental ill health and inform treatment. So a formal definition would be the moment by moment quantification of the individual level human phenotype in situ, using data from smartphones and other personal digital devices. Ah, OK, and there's a paper if you're interested. I think that's like the first paper to ever introduce or use the term digital phenotyping from about 2016. 0 yeah, it says it here 2016. Another term is personal sensing. Perhaps that's a little more intuitive term, uh, so to quickly break down that that definition digital phenotyping in an etymological sense. Phenotype is a scientific term used to denote an organism's naturally observable physical and behavioural characteristics. Thus, digital phenocopy is simply the process of measuring or identifying certain behavioural characteristics based on an individual's digital footprint. Um, and there's a term extended phenotype two. I think that was introduced by Richard Dawkins. Uh, so the digital footprint is like in a in a general term, you know, um, the digital trails from our various interactions with whatever devices smartphones, wearables, you know, that ring that you can the uru ring that you can put on to measure your sleep and that, um, using just a laptop to interact with the Internet and various other um, possibilities, such as like, um, what is it? Internet of things? Devices? Um, but we'll focus on smartphones because that's like the most done thing. Um, so the use of smartphone digital phenotyping is that the usage in the usage and sensor data from a smartphone can be collected to analyse the infer information about its user. That much is clear so far, but I'd like to also establish that there are two types of information we can classify. One is contextual or situational information to learn about somebody's situation based on their smartphone information like I can obviously work out where they might be located, roughly speaking, by looking at their GPS or what type of place they're visiting. Or you can even work out, like, what mode of transportation somebody's on by measuring how their phone is vibrating or how fast their phone is moving. For example, uh, and then there's the behavioural type of information, like, how often are they unlocking their phone? How fast are they typing? Um, how many times are they making calls per day? So this is the spiel for using digital phenotyping. Uh, traditional mental health care is resource limited clinical sessions and patient self reporting can be affected by certain subjectivities in assessments and accounts of behaviour. Existing clinical practises are inefficient in detecting early stages of mental illness, and standard methods are based on face to face interactions and assessments with clinicians conducted at set times and locations. Once the clinical therapy session is over, there is no sufficient way to monitor a BA a patient's behavioural patterns in their real day to day life that could be valuable information. And finally, continuous smartphone sensitive information, unobtrusively collected can fill this gap providing value to both patients and clinical care providers. So that's like the spiel that you know one would make. I suppose if you're trying to sell this thing, um, because, yeah, if you're gonna just see your therapist once every two weeks, you you're doing things in between that time and that stuff could be relevant, you know, for the next clinical session to give the clinician some insights in how to how things have been going and perhaps Richie insights and just asking somebody to recall salient events over their last two the last two weeks of their life. And then once you collect all that digital footprint information you can, you know, get machine learning or possibly LL MS these days to analyse the information and provide snapshots. Um, so just to give you AAA more detailed idea about some of these sensors and how they're used there's like the movement and physical activity sensors like GPS accelerometer gyroscope bit more, a bit more of an obscure sensor. But it helps to determine you know, the orientation of your phone. These sensors can be used to distinguish sedentary and non sedentary states and to investigate connections between movement and conditions such as depression. Um, some of the early results in this field suggested or have suggested that people who are depressed tend to have less spread in the locations they visit. Um, let me see if I can quickly just open this PayPal. No, this is I. I can open it, but it's not the one I had in mind, but you've got a paper like predicting depressive symptoms using smartphone data. Um, and what's basically done in a study like this? Probably they just, uh, collect the data. They generate a bunch of features from that data. Like, if you if you click raw Jeep jump, Geo geo coordinates like a latitude longitude coordinates for for somebody for a month. You know, you can work out, like, for each day of that month. How many places they visited, how much distance they travelled. Um, how much variation there was in the the spread of their their travel, all those things. And then you kind of use those to, like, maybe predict, Um, a mental health score measure that you're taking. So during that period, you might like once a week, uh, administer like, I don't know if anyone knows here knows about, like, mental health questionnaires like the patient Health Questionnaire nine. Or it's called the PHQ nine questionnaire, which is like nine questions that ask people like, on a scale of 1 to 5 rate how you felt about this and that. And then you, like, get a total score from that questionnaire and that total score like then becomes something you can try to predict using a machine learning algorithm. Um, and your independent variable inputs would be the the smartphone sensing features you've generated. This is an interesting one. Um, one small study found some modest or inconsistent relationships between the time spent in certain locations of mental health such that depression and anxiety scores were lower. For some people who spent more time in spiritual locations and nondepressed, people spent significantly more time at work. This is like a small sample size, and nothing conclusive or definitive at all could be Dr Can, um, obtained from this study. But it's just an interesting possibility as well as like like I said, measuring like quantitative things like clusters or Total distance travel those numbers as things to predict, just also getting like the semantic places somebody's been to what types of places have they been to, and then just doing like an analysis with that type of information? Um, you know, the people who spend more time at university feel better about themselves. Maybe that's possible. We're actually looking at that at the moment. We did a digital phenotyping study me and a PhD student last year, and one of the features we generated was time spent at the University of Melbourne just to see if that, like, led to any better results or something. But we don't have the, uh, conclusive analysis yet I

SPEAKER 2
would have hypothesised negative outcomes myself, but OK, let's see

SPEAKER 1
how that that goes. Uh, we've got the communication and social engagement types of senses or things. Um, we know that there are connections between social connectivity and mental health, like so prior to this all these digital stuff. That was just a thing where there've been studies and it just sort of stands to reason. Uh, social connectivity is good for mental health and well being and with digital phenotype. And we can quantify this. We can measure it in a sort of, um, digital way. And we can even like S forecast go. Oh, this person is dropping off their social engagement. Maybe some type of social intervention might help them. So we have things we can measure with a smartphone like social media app, usage, S, MS message accounts, calls even like, more fine grained level like incoming outgoing, unanswered answered calls and working out the ratios between these things, Does a person receive more incoming or outgoing calls or, like, do they not answer a lot of their calls? Why might that be? Do they usually answer their calls and then, like for one week, they've just completely stopped answering their calls? Might that be a clue about something. Um, one study found that further reductions in the number and duration of outgoing calls, as well as the number of text messages we're associated with relapses of schizophrenia in a case like this. It shows, though, that sometimes, like you might have a model a digital phenotyping model. But it will only apply to begin with to a certain cohort, right? Like, you know who've already had, um, an initial episode of psychosis or something, so you can really only apply the model to them, but because that's what the model is based on, Um, maybe for, like other people who don't fit under this category, this would do nothing. It wouldn't mean much. But for that cohort, it did mean something. Um, another interesting one is Bluetooth, like where work has been done to work out, like how much somebody is socialising or how much you're in social context By inferring this from like their Bluetooth information. If you're never around anything or anybody else, you're not gonna have any other like surrounding Bluetooth device signals in your digital footprint. But if you go to a lot of parties, you probably would have a lot of Bluetooth, um, signals in your, uh, digital footprint. And then we have this keyboard and screen interaction sensors in that an individual's tactile interactions with the screen, such as typing press Swipe in um, one study showed that both average delays between keystrokes and auto correction rates IE misspellings correlated positively with a common depression scale. And then, in a in another study comparing two groups, one with depressive tendencies and one without the depressive group showed longer periods between pressing and releasing the key. And this indicates a solar motor reaction time or psychomotor retardation, which is a feature of depression. I think this is one of the mo more promising like senses. Um, also for like, more clear cut conditions in a way like, um, clear cutting a sense like, um, uh, motor neuron conditions. Or, you know, cognitive degen neurocognitive degeneration conditions, where over time you might, like see some type of degradation in somebody's typing ability by measuring using like longitudinal machine learning analysis of their typing history. Ryan. Yeah, so I don't know if you're gonna talk about

SPEAKER 2
this, but what comes to my mind is, uh, issue of like disparities. So, like I'm on my computer all the time, so we'd be able to get good baseline about my, um, behaviours and then observe changes from that. But what about like a plumber or worker? I don't know if you're going to talk about that later.

SPEAKER 1
No, no, it's a good point. I mean, whether I'm talking about it later or not, it's a good point. And that's you should yeah, definitely say things like that if they come to your mind. Uh, my answer. Yeah, you're right. You're basically right. Um, I think it leads to the point that there are two types of ways that this can be approached. One is like to have, like, general population informed models that then, like, might be applicable to the next person. Right? So, like, I've got a new model that's trained on 10,000 people, right? And then, like, if the next person 10,001 person comes along, I can just apply that model to them. That's one way to do it. Then there's just like the possibility of developing models per person, right? So I like get six months of data from a person, I suppose, and I build my model just on that for that person. So that then after that six months, I have a model, and I can I apply? I can apply it to their seventh month or something. So if I see something abnormal, you know, maybe it will Will show us something. Does that make sense? Yeah. Clay.

SPEAKER 3
Sorry. Me again. Um, with these, um, studies are they? How do they get the data? Are these where you've sort of consented people to participate in the study and collected their smartphone data? Or is there a way to access kind of stuff from Facebook or other?

SPEAKER 1
Sometimes this is done with Facebook data. Um, but how would it be done? So Facebook less so, um, because, like, Facebook doesn't really allow this as much anymore after the Cambridge Analytica scandal, Uh, which you may be aware of, Uh, Twitter technically doesn't do this as easily anymore either. Since Elon Musk shut down the API. Unless you pay a small, you know, arm and a leg to get that data. Um, but yeah, previously, like a source like da um, Twitter. Or maybe you've heard about Reddit, which are like these forums which have you know, if you're gonna post it to to Reddit, you acknowledge it's gonna be public data basically because they're public forums. And so then people like tap into the, um the reddit API and, you know, scrape a whole bunch of posts from, like, say, a depression or a PTSD forum where they talk about these topics and analyse them. The issue there is that you've got the input data, but you don't know who these people are. And you don't really have any clinical measurements about those people. So it's kind of only half of the pitch of that type of data. If you're gonna run your own study, what you would do was yeah, you have consenting participants. Who you recruit yourself. OK, So like, when I did that smartphone study last year, I, I told you it was like, um University of Melbourne students. We recruited about 150. We went through the University of Melbourne ethics approval process. You know, there was a plain language statement and we advertised the advertisement around, and then people came to us and said, I'm interested in participating, and then I went through a very rigorous process we installed a tool on their phone. Um, I'll just quickly show you that tool. So this is aware light, which is, like, our kind of homegrown smartphone sensing app. And, um, you can see, you know, some of the sensors here. It's quite a lot of sensors. And then we installed on participant phones, and, um uh, they can contact us. They can, you know, um, they can opt out whenever they want to. And they can also turn each sensor they can turn on and off at will. They're in complete control over what they want to share, and then the data basically goes to, like a University of Melbourne database that we protect. And, um, that's it is, um any other questions? Um, which is easier to do out of those two? Uh, well, it's easy just to get the existing data from Reddit or Twitter. This was we had to do a lot more groundwork here. I mean, I even helped my student walk around the University of Melbourne handing out pamphlets during orientation week, so yeah, you've got to work to get your own data. Yeah.

SPEAKER 2
So, Simon, are people ever I mean H How aware are people of how much data can be collected about them. And so you are informing them that with this app you're collecting, you know, XYZ um it's I. I would imagine most of us would be shocked if we actually sat back and thought about what you know probably is being collected by the phone providers, et cetera.

SPEAKER 1
This is a good um, that's a really good question. And it makes me think of a little sort of study to do whereby. You know, you kind of recruit 200 people for 100 of them. You just do what we did like you. You just list the sensors and, you know, these are the sensors we're gonna collect data from OK, and you explain, You can opt out if you want to. For the other 100 people, you list the sensors, but you also give them a screenshot of the actual database tape. You know of what it actually looks like for us to collect it. Like here are your geolocation coordinates. Here are your keyboard, you know, inputs and things. And with those 100 who we show that to be more likely to end up backing out at the last minute. I suppose that's an interesting question. I mean, I don't have time to show you some of these data, but I could easily just go into the the MySQL browser that I have, you know, because it's a MySQL database and show you what it looks like. And some of them, like things like phone calls. Well, that's not that privacy invasive because it's just like, how many times did a person receive a call? How many calls did they make? But the ones like, um, keyboard input. You know, what are the people typing into their their keyboard? You can mask that. Like, if you don't mask it, We actually get the characters they're typing into the keyboard one by one. So if I type Hello, Bob, we can see that. But if you just want to measure typing dynamics alone and you don't care about the character content, you can mask that so that every character just becomes the letter A So you don't know what it means? Leah?

SPEAKER 4
Yeah. Yeah, Thanks. I really interesting lecture. Um, I guess I sort of just had a little comment from earlier. Um, I've noticed that, like, my phone will tell me weekly like how many hours I've spent and also in comparison to, like, the previous week. So I think after semester one, like the first week of holidays, it was like, you've you've been on your phone, like, six hours more than the previous week, and it's like, Oh, OK, yeah, that kind of makes sense. Um, but yeah, I just sort of I think it's interesting that, like, Yeah, it would nudge me like that information, but it's just telling me like it's would be up to me to, like, interpret it or like to take any action from that. Um, but yeah, it's very fascinating to see that that list that you've just put up with, like, how much data is actually being recorded.

SPEAKER 1
Oh, sorry. Go, go, go, go.

SPEAKER 4
Yeah. So, um, like, this is all this information that you can track, and you've done this in, like, the the sort of the research. But are there actually apps that are like, Does this exist like these particular apps for digital phenotyping? Uh, so I should Yeah, qualify some of these.

SPEAKER 1
Um, So what we're doing here is quite hardcore like this is our tool. And, um, we're doing it for research. This type of stuff. If you have an app like aware light, you can, um, basically, um, hack into, like, the android system to get this this information basically, um, but generally, apps don't do this right. This is just a re very general research platform. And a lot of the time, not all of these sensors are turned on for a study anyway, So that's the first thing to say. Although it's possible to get this app generally, people don't. And I should also point out I'll show you here. Actually, it's a little amusing. Um, so this is like the aware home page. Yeah. You won't find a wear on the Google play store basically, because we hack into too many sensors, they won't host our app on their official store. So there might be some comfort that Yeah, Google's obviously mindful of this, and, um, will, every time an APP is uploaded, you know, to be included on the Google App store, they'll vet it to make sure it's not doing all of this. We're OK because we're just doing it for, you know, genuine research purposes. We have no malicious intent whatsoever. Um, but you're right. The other thing. I should say that when somebody instals a wear and is about to kick start it, they have to consent to it. And, like, they have to, um, uh, provide permission for the phone to access all the services and sensors. All that. So, Yeah. Ultimately, the user is in complete control. Yeah, Cool. Um, except for that one. You told me at the start about the, um, like, how much time you've spent on the phone that that's a pretty, like, uncontroversial one. It's just very easy to gather just by, like, measuring how long the screen's open for Yeah. Yeah. So that's a pretty basic one that anyone can do, Kind of. Yeah.

SPEAKER 2
Yep. Olivia Metcalfe. Right. That's Olivia's last name, right? At last. I guess two weeks ago at the A. I, um, summit or whatever. The CDT HS, uh, thing was, um she brought up, like, kind of a you know, concerting, uh, issues, you know, measuring heart rate. And, you know, she was just kind of like putting out, like, some of the nefarious things you can infer and that wonder if people are aware of Well, so her example is like, you know, you could use heart rate to estimate Oh, someone's having sex. And you could you could do that with geolocation, you know? Oh, they're they're not at their house. They're they're at this other house that they go to, you know, you know, So just a lot of, um, you know, the kind of things that you can see is problematic because I think she was kind of raising, you know, the concerns of, you know, the ethics and whether even the ethics overview was, you know, stringent enough. Um, just yeah, I'm aware of those issues because those

SPEAKER 1
cases have actually happened before. Um, Well, what I would say that there are two things. One is that there is gonna be some exposure just by the very nature of this thing. If you want to have personalised medicine, well, you need to have personal data about a person to some extent, and that's a the A trade off that the individual will make. How much do I want to share versus be like a cost benefit analysis? Um, in terms of research, I think our endeavour should be to, you know, develop models that can have, you know, some type of validity predictive utility, unlike as little as needed. So we collect raw data and then just get the information we need from that raw data and discard the rest. My simple example is if you can develop a model that can detect like depression based on, um, qual uh, acoustic qualities of the voice like pitch and intonation or something like that, or delays in voice. Then record the person speaking, extract all of those acoustic features from from the recording and then throw out the recording because you don't need to know what the words are, what the semantic content of the person's conversation is. You just need to know their pitch and intonation and frequency so kind of a minimum necessary data set a minimum. Yeah, to have rigorous frameworks to enforce minimum necessary data sets exactly. That's a great way to put it, so I'll move on. So just to summarise with some opportunities, tapping into this rich array of sensors in smartphone can be used in two ways. In terms of mental and health. That's mobile health development of a digital phenotyping framework to analyse an individual's accumulated smartphone sensing data and converting this analysis where possible, into meaningful psychological information, including prediction, detection of mental ill health but also provide situational or contextual information about a user which may be of use in determining relevant real time digital therapy delivery. And this is a segue on to my next point. This whole notion of Detective Delia Detect is the use of smartphones to sensors to detect psychological and situational information about smartphone user as I've been going over the other half is a delivery using this detection information to personalise real time delivery via push notifications of therapy actions in that beyond simply offering and expecting users to visit a website to help to meet them in their situations and dynamically deliver via smartphone, relevant therapy directly to their pockets and some technical terms or some industry terms we can give. These things are like ecological momentary intervention or, just in time adaptive interventions. And there's just an old link to a position paper I wrote some years ago like outlining this idea because I was working on this app and, um, you know, it had like 100 mental health mini modules. You could do 100 exercises, and we were just expecting, like, I suppose, users to get onto the site and to browse the modules themselves manually and to go. Oh, this one looks relevant to me. I'll try it out. And I'm just thinking, Well, if you can, like, get somebody's digital footprint information to learn about them and to try to work out what might be the most relevant therapy module for them to do, you could kind of D out this cool, dynamic, recommender system that could make recommendations to a person based on what you've learned about them from, like digital phenotyping. It's similar to like, uh, a recommended system for those who haven't heard of them. You think about like YouTube. They recommend videos to you or Amazon recommends books or Netflix recommends movies, and that's all based on a recommended system based on, you know, users you're similar to, or you have a similar profile with, or your viewing history or any other types of preferences what types of movies you've given five stars to et cetera. Well, with this type of digital phenotyping into mental health apps, you can learn a whole lot more about a person like, um, where they might be, Um, how they might be feeling their mood. And you can deliver these. Like I said, ecological, momentary interventions. Which means like delivering a short intervention in the moment based on the, um, profile. Um, yeah, that's a link for those to be interested in that idea. So moving on, I want to talk about natural language processing for mental health insights. This is another very interesting area, basically in line with the idea that the language we use provides a window into our minds. One's words could be analysed with natural language processing to infer mental health information about them. OK, we have various sources, such as clinical transcripts, you know, transcripts from psychotherapy sessions, social media posts, keyboard or keypad input. Like I said, with aware light, we can get character input, content or even voice assistants. You know, if you're talking to a voice assistant, could the voice assistant, you know, learn about you by analysing the way you speak something like that, just to give some examples like I did with digital phenotyping, um, psychosis. I used as the first example because that's actually how I kind of got into this whole idea about a I and mental health by reading this interesting paper where they were using, um N LP to, like predict psychosis relapse in that some early research on NRP mental health used semantic coherence and tactic complexity to predict later psychosis development from patient clinical transcripts and found that the use of less complex, more incoherent language, um predicted subsequent onset of a psychosis. You know, um, episode and I go in a sense, Well, that's kind of obvious. Um, people with psychosis, you know, their their language become their language becomes less coherent, and their S, their their vocabulary, their sentences become more impoverished. But I suppose the interesting thing here is to be able to do this in a very like, data driven, quantitative, rigorous way allows for more facility in that, like the machines can just be in the background analysing somebody's language and can provide, like early prediction or early notifications about the possibility of this happening, thus facilitating early intervention. If it's just up to humans, they might not pick up on some of this stuff, you know, before it's too late. uh, depression is another one. In one study, the Facebook posts of a relatively large patient cohort were analysed to predict with an accuracy approximately matching screening surveys like traditional screening surveys, depression as recorded in their electronic medical records. The researchers found that language predictors of depression include emotional, interpersonal and cognitive processes. Things such as sadness, loneliness, hostility, preoccupation with the self rumination. Um, so you know you can. There are tools like the one tool is called LIWC, which stands for linguistic inquiry word camp. And, you know, you can give it some text and it will give you like a score between, like, zero and one of, like, how much of something or some type of thing A. A text has. Um, actually, this is a funny one of maybe I'll go to it later, but you know, So it's interesting to see um, you can one of like, uh, two of the categories that it gives you a score on, for example, are like, what percentage of third person pronouns does this using their this person using their conversation versus what percentage of first person pronouns like we versus I and like they've shown how a shift over time. A. A person who normally has, like some type of normal distribution, then just starts suddenly using way more many first person pronouns than normal could be like an indicator of something things like that. Or you can measure, you know, the sentiment of the person's pros and see if there are any significant shifts in that over time and various other things. If you're interested, you can just google IWC and you'll find it. Um oh, yeah, you can also use language to see more obvious possibilities. Um, like harm self-harm detection and things. And Facebook once tried to do this. I don't think they got very far with it. And I'll just mention that we can go beyond linguistic content. Where beyond, um, the N LP processing of text. There's like work on the audio analysis of the acoustic may. Maybe the simpler term, or it's like term the paralinguistic parts of speech, such as volume, pitch and intonation. And it's shown, um and using a. I has shown that such properties of speech speech can also be computationally analysed to infer mental health information. I mean, it's not, it's out of the scope of this lecture to go down. You know this topic in detail, but there's a good review paper here on this topic for those who are interested. And I'll also talk about chat bots. Um, people have heard about chaps. By now, it seems like they're becoming more of a F. There were a thing like in 2017, and then there seems to have been a lull. But now they've come back. And we, uh, I mean, that's been given even more of a boost with, you know, chat GPT and all the large language model stuff where you can do your own chat like out of the box much more easily these days without having to construct it yourself. But let's talk about them in relation to mental health, which is one of the areas that they're really kind of, um, talked about. So we kind of know what a chatbot is. Basically, it's a computer programme that mimics conversation with users via chat interface, either text or voice base. There's various ways that they can be implemented. You can have like simple rule based ones or pattern matching to advanced machine learning and N LP techniques like large language models and generative A I I will point out that irrespective of the actual intelligence of the responding Bart, there is something distinct about the experience of I use an entering input and a bot responding. I sometimes feel like it's a bit like a slot machine. Um, you know, you input something and you don't know what you're gonna get back. Why are people so drawn to chats? I don't exactly know why we know that at the other end of the thing, there is no conscious sentient agent that can kind of empathise with us or consciously cognize what we're saying and respond as a human. But, you know, people get drawn to them. There is a range of uses for chapati mental health. The simplest way is to just have, you know, a chap on interface is a conversational search interface like therapy content. Um, that's not really that interesting, but it's very useful. Um, wh chatbots are far off from being able to satisfactorily emulate language use and replicate psycho psychotherapeutic dialogue. They can now maintain basic forms of conversation. I mean, it's getting less space now with the you know, the uh, cutting edge generative A I systems. Um, but I will say that, you know, many if not all of you have used chat EPT by now, but it's still definitely not solid enough to be a reliable mental health therapist. Um, one use of such a chat bot could be to guide users to exercise in the mental health app. So here are a few examples I'll click on some of those links examples of mental health apps that incorporate a chatbot setup. So they they they they're sophisticated mental health apps. But they they're not just like wild systems like chat GP T, which are, you know, just these stochastic parrots that you don't exactly. You can't be too sure of what they're gonna say and, um, are a bit a are a bit wild. Still, these systems are like largely rule based with maybe a bit of sort of machine learning type. Um N LP conversational agent aspects. But they are still, like, largely curated by the developers. So robot is probably the most popular app that's out there. It's quite it's I would say it's definitely the most popular mental health chatbot system. Um, and let me see if I can quickly find some images of chatbot. There you go. So that's the little icon mascot. And this is like what a conversation might look like. Um, but you can see here that after the conversation occurs with this part of the conversation, like it prompts you with, you know, one of three options. So it's still guided. It's like rule based or the inputs are based on predefined sets of options. Um, rather than just, like, chat GP T where you can type in anything and you don't know what you're gonna get. Exactly. And there have been some controversial cases with, you know, some of these more open ended systems. I might be Have some of these coming up. Uh, yeah, like replica is a good example. Um, might have Yeah, I'll go. I'll go to this. Actually, let me just stop my side for a second. Get some other slides, give us a chance to have a couple minutes break. Anyway, if you have any questions, please shoot so back. So I should mention that the history of chaps is intimately tied with psychology and that the first well-established chapli known as Eliza created in the mid sixties by this guy was created to simulate a Algerian psychotherapist. Does anyone know what Rogerian means as an adjective? No. OK, that's fair enough because I didn't know what it meant either. When I first came across that word, it means, um it comes from the the person Carl Rogers, who was a pioneer of psychotherapy, one of the most influential important psychologists of the 20th century, came up with this, like, human centred psychotherapy approach. And yeah, basically, around that time, Rogers was was big. And I think this guy goes, Well, I need it. I. I don't have fancy. They didn't have fancy N LP stuff back then. They just needed to have this simple tool that, like largely if, um, how would it happen if the user inputs a sentence? The bot, just large, like, responds back to the user. Why do you feel this way, or what do you think about what you just wrote? So it's just like that kind of classic def, not reflection type of psychology approach, which was suitable at the time, because the N LP wasn't that great. Um, so this would be one example of I think of actually a conversation that happened. Um, this looks reasonable, I suppose. But there are examples of it going completely off the rails. I might have some of those examples coming up. Yeah, this is an interesting point that why? And Bob did not attend for Eliza to be an actual chatbot therapist. People think that they go. This was the first Cha Chatbot therapist. He created Eliza as satire in a sense, to explore communication between humans and machines. And he actually doubted he was sceptical. He doubted that computers could simulate meaningful human interaction. So was surprised and shocked that individuals, including his secretary, attributed humanlike feelings to the computer programme. Like there's a story where his secretary asked him to leave the room because she wanted to have a one on one with a chat box. And this led to this notion called the Eliza Effect, which is the tendency to to project human traits such as experience, semantic comprehension or empathy into computer programmes that have a textual interface. And I've talked about this, um, and the spot replica I think I had in my original slides. Yeah, so replica, um, is one of the most sophisticated examples of a chatbot capable of emulating, empathetic in quotes, exchange. But it's certainly not without its issues. Unlike like Robot, which is curated and, you know, controlled and has safeguards, Replica sometimes is just like a bit wild. And the sponsors are completely left up to a kind of stochastic machine learning, you know, LM generation process. And, um, yeah, like, here's a story. I think what happened. Like they changed something about the BOT, which meant that its personality changed with meant that some of the people who were gelling with that personality overnight just lost it in their interactions with the BOT. And it kind of affected them. Uh, a I based companions like Replica are harmful to privacy and should be regulated. Um, I tried the replica A I companion and can see why users are falling hard. The APP raises a series of ethical questions. Um, if anyone here is interested in this type of stuff a bit more just Google replica issues with Replica and and you'll see all these stories so I maybe we can talk about that a bit more at the end if we have time. But I'm interested in chatbots, but to try to use them as some type of replacement for therapists. I think that's a big issue. And I Seriously, I think rather than working out how we can replace humans with chap arts, I think we should simply work out the contours and the boundaries of how far we should use them. What are the sensible boundaries of them before we cease using them beyond that boundary? Um, and about chat GP T. Um, yeah, whilst you whilst it could be used to obtain basic mental health information and advice, probably best not use it as a substitute therapist. Um, as I said, um, and ultimately, yeah, a chat that can carry out a proper psychotherapeutic conversational session. A replicate human therapist remains to be seen, if at all. Uh, if at all entirely possible So that's that. And now I will talk about some ethic points. So actually, let me see if I can get some more slides. Here are some more? Yes. No. OK, ok, so the final section we're going to talk about some of the ethical and human computer interaction dimensions to digital mental health and a I and mental health things like the ethics of APP design, privacy and confidentiality and digital therapeutic alliance. So, uh, mo, motivated primarily by commercial and advertising interests, many websites and apps incorporate features that are intentionally designed to hook users into Max in maximising their attention and usage time without due regard for the quality or benefits to well being of these usage. So beyond clinical efficacy, efficacy considerations and also in concert with them, we must be mindful to ensure that the system in general is not detrimental to well being, and that features and components of little value or that have a negative impact on psychological health are are avoided, and also that to develop content and features that promote mental health and well being and that users are motivated to use for good reasons. Um, I don't know if that goes without saying, but yeah, I think it's important. I kind of came to these points when, say, around 2018, I was working on this mental health app and back then, like this kind of whole problem with social media was becoming more and more parent. And, um, you know, 60 minutes started running news stories and that about, You know, the problems with Facebook and how it's, you know, hooking people and just some of the dodgy things that some of these sites are doing are like Snapchat, for example, um, and some of some of these features of these systems that are, you know, designed to people in So it maximises the time that they're looking at those systems or scrolling through news feeds. And then I was working on this system, this mental health that was like a social media. It was a mental health app that had, like, social media aspects and all these other aspects and thinking about how do we do this without doing it improperly in an ethical sense. And I think that the APP should encourage and facilitate offline activity outside of the site. So it shouldn't just be about staying on the app, but just you know, about touching in with the app and doing what you need to do on the app and then getting out of the app and doing healthy things outside of it. Um and that online activity should be promoted within the site or the app, insofar as it is something that is beneficial and inherent to the site or app or can genuinely only be mediated or facilitated by the app. Um, so that's that. I mean, I could go talk about that more and more, but yeah, basically, you want to promote overall well, being with your app as well as, like, get people to use it for what they need to use it for. So this type of stuff is pretty generic. Um, can these apps pose a risk to patient confidentiality? And we've kind of gone over that other bit with the digital phenotyping stuff. So security is a factor. Um, and you got to ask questions like, where is the data being stored? What? Security measures are in place to secure the data on the patient phone on a remote server. So this is actually another one, Brian, Like we're talking about the minimum required data kind of idea, but also what can be done on the phone, I suppose. Can we collect the data on the phone, do the crunching, So at least some crunching of the data on the phone so that it needs never needs to be uploaded to a central repository. So one thing that, like some of us, What some of my connections in the CISHC I group are doing are exploring large language models that are just on the smartphone. Um, and you can, you know, maybe analyse the data on the phone itself. Um, how can the data be stored to maximise unidentifiable, which is what we were kind of seen before. And this is also what we were saying before. Therefore, users should could should have control of their data. And I already mentioned this before. I wasn't planning to mention some of these, but they just came up in the course of the lecture, so that's a healthy sign. Um, And like I said, giving uses complete control over things. I will mention that with that, like, you know where I was saying how people can turn sensors on and off whenever they want to. We actually store that fact itself also. So every time somebody turns on or off a sensor, we we record that. And that itself is valuable. Secondary meta information, like can kind of say, Well, why is this person suddenly turned off something? It just might tell you something about You know what? You know they're about to do, I don't know. But it could potentially be useful meta information. And now I want to talk about the Digital Therapeutic Alliance. This is an interesting topic. Uh, it's kind of dear to my heart because, actually, I have one of my PhD students. She's doing her completion seminar on Wednesday, and she kind of, um he's working on this topic after myself and a colleague started a few years ago, and it's something like this. So the therapeutic alliance is the relationship that develops between the therapist and a patient and is a significant factor in the outcome of psychological therapy. So psychotherapy, you know, was developed over the 20th century. You started off with Freud, um, you know, who just was one of the the fathers of psychotherapy? Um, and he goes, you know, a kind of thing develops between the patient and the client, and he also had the notion of transference, which is more problematic. And then after him around the middle of the 20th century, like you had Rogers Carl Rogers, who I was saying was a pioneer in the 20th century of psychotherapy, and he's talking about things like the relationship between the client and the therapist and things like having empathy having, um, positive regard for, you know, the the the the clinician the therapist has to have empathy for the client has to have genuine, like concern for them and what's called a positive regard for the client. All these things, these successful ingredient or ingredients for SEC successful psychotherapy and then, like this other guy called bordan, like 79 whatever came up with his own more specific notion of therapeutic alliance, and his conceptualization has three dimensions. Um, one was like bond in that there needed to be a bond that developed between the therapists and the clients. The other was goals in that they need to establish an agreed upon set of goals that they want to achieve in the psychotherapy. And the final one is the tasks. What tasks do they agree upon? The client needs to do in order for the goals to be achieved so as mental health issues, therapeutic alliance, as mental health care, starts to increasingly adopt digital technologies and offer the therapeutic interventions that may not involve human therapists. The notion of therapeutic alliance in digital mental health care requires explora exploration. So we have questions such as What is the nature and role of the therapeutic of the therapeutic alliance in digital mental health solutions? Does the traditional notion of a quality relationship between client and therapist hold true in the digital environment and do aspects of the traditional T? A. That's what we call it the T A have digital analogues and what novel aspects emerge in the digital realm. So ways who theme can be explored this umbrella term of digital therapeutical violence? The easiest is like the standard patient therapist alliance in the case of tally health therapy sessions and that has been studied quite a bit now like, Well, if I've got originally the face to face in person sessions, what happens if I have, like a session over Zoom? Does anything really change there? And it seems that by and large, alliance to a decent extent survives or to a good extent survives in the teletherapy session, you might be sad on some things, like even though the language communication, the dialogue is preserved, there are a lot of non linguistic aspects of in person, face to face, you know, exchange right like body cues and subtle, you know, body signs, whatever. You know what I'm talking about? So to what extent there is a gap, Maybe that's something that requires more exploration. We're kind of, however, looking at this type of thing, we're like the relationship between a user and their mental health app, including their like, standalones fully automated smartphone or, to the next level, the nature of the therapeutic alliance in anthropomorphic digital health interventions, including chat bots and virtual human therapists. What does is there like? It's It's an interesting question. Um, what does it mean to have a bond with, I suppose a chat bot. You probably can't even have a genuine two way bond with a chatbot because a chat bot can't reciprocate. But you might be able to have some type of wide way connection with the chatbot, I suppose. And that's really interesting human robot interaction. Um, so that's that. And if I could skip into these slides Uh, yes. So, yeah, if you're interested in these topics, I might be able to actually post some of these slides. Also on the L MS. Um, let me see. OK, these are just some papers that cover this. They're only like a handful of papers that have been written on this topic of digital therapeutic alliance simply because it's quite new. Um, I'll leave those there for now. And, uh, this is not really like this slide here is more so intended for Well, I give, like talks to clinicians, you know, psychologists about a I and mental health. So I'll just say it anyway, because it's an interesting one. And this is what the words that I leave them with. With the advent of digital data approaches to mental health, what could be termed the Mental Health Analytics or the more general psycho informatics? That's a nice term. There will need to be open mindedness among practitioners and paradigm shifting mental health care and upcoming generations of mental health practitioners may very well require a new data savviness, and I think it's about augmenting the capabilities of humans in that the human plus machine is greater than either alone. A. I won't replace humans. Humans using a I will replace humans not using a I. Um I'm not sure if there are any mental health clinicians in this audience, but anyway, I thought that was an interesting slide. So I'll stop for now and open it up to any questions or conversations people might like to have. Or are we done?

SPEAKER 2
Well, surely we have some questions. Well, while they're formulating their questions, Simon, um, when you were talking about the the therapeutic alliance, um, something came to mind that I think I don't think is really relevant, but I think it kind of raises things that, um, you know, might relate. So I saw a YouTube talk about tools like chat, GP, T, um, degrade online communities, and the and the specific communities they were looking at were like computer programmers. And so you you because they are no longer going out and asking their peers questions or developing those kind of relationships, they just go to, um, chat GP T get their answer, how to write, you know, such and such a code, and then, you know, continue on their tasks. Um, it seems like the, uh, mental health really largely occurs in very scarce, you know, time scarce, you know, kind of settings. Um, so it seems hard that this would threaten, you know, the the use of an app would really kind of threaten a therapist unless the person dropped. Um uh, you know, seeing a therapist. Um, but I, I you know, maybe this is just your last point. It seems like that what would what would you would expect to evolve is how does the therapist learn to make use of the generated, uh, chats that are, you know, done? You know, in those two week periods in between, et cetera, But just kind of your thoughts about. And if if that triggered any thoughts to you Uh,

SPEAKER 1
well, firstly, about that degrading online sort of conversations or forums is that one of the points you made? That's an interesting one, because I'm actually like examining these. I'm examining the thesis at the moment, and one of the papers is about, like, exploring how people perceive the introduction of like a chatbot into an online forum. Does it help having it like it can be good for some things like it can summarise chats, Or it can be provide a Kickstarter sometimes, But they also caution about it, just like you know, if there's too much of the chatbot, it just descends or degrades the quality of the interactions in the forum and stuff. So I would just back that up with that point because I happened to read that paper last night. Um, otherwise are you talking about, like, the kind of I suppose knowing what this type of thing is and what it isn't And a human? What they need to be educated on is how to use this. I suppose. Um, like, should we talk about using LL MS in this domain? Yeah, I I can only think that if I was a were a practitioner, I would say, Have a question, maybe. And I will ask T GP that question. And then I would would Is it appraise the question or the the response and incorporate it into my thinking? Maybe, um, is what I'm saying. Relevant. What? Yeah, yeah, I, I I guess.

SPEAKER 2
You know, on the one hand, I think, you know, if I were to engage in, like, a, you know, chat GP T or cloud based, you know, psychotherapist chat. Um, those transcripts might be of use to when I actually see my human, you know, human show.

SPEAKER 1
So So that's one thing I'm I'm actually currently, uh, you. I think you might know these I. I showed you that project idea once that like, um, just, uh what what What are we doing? We're creating a digital twin system. So, like, say, a client has a therapist, OK, I'll start off with getting the initial information about that client like their, you know, demographic information or whatever and some type of initial assessment. And then, you know, they might have five sessions and and plus more so just say like they're they're gonna have 20 sessions with their client. Like you start off by feeding the initial information into an LLM. And then with each session, you've got a transcript of that session. You add that to the the knowledge base of the LM for that person so that, like, after five sessions, when it comes time for the next session, the sixth session, the clinician could go. Hey, I'm about to have a session with you or something. What do you think? You know, you might want to to talk about today like you can basically get the lm to somehow like, be a digital twin of the real client based on you know, whatever information you can feed it and get some insights that might facilitate or inform your treatment of that client. Hey, Wendy. Uh, Claire, I think always it's it's it's me bugging

SPEAKER 5
you this time instead of Claire. We're tag teaming. Um um, I. I guess I find it curious, especially in this mental health area that devices and apps have been linked to a significant decline in in mental health, especially in in younger people. So So why do we feel that we can turn it around and actually use it for good? Um, rather than make them feel, um I guess more isolated and and turn even more inwards towards a computer.

SPEAKER 1
This is a great point. Um, it touches upon what I was saying with the ethical app design stuff. So firstly, um, I mean, if you've just got those stand alone, uh, I think with apps and that I think largely the problem is social media. I think you would probably agree, Um, so if you've just got a stand line app that serves some functional purpose, like keeping track of some, you know, useful health information or mental health information, that's probably a good thing that, you know, isn't susceptible to the types of dodgy things that can happen with social media apps, for example. So I suppose the more ones that do need that type of creation are like there are the mental health apps that incorporate a social element to it. So, like engaging with peers on a social network, Um, you know, or, you know, with people with similar shared lived experience. And I would say simply, there is the The onus is on the the APP developers to go. OK, we're just kind of using some type of social media app essentially for mental health cohorts. How do we do this in a good way? That just kind of uses the good portions of what this technology can do and doesn't use the bad portions? Because so, I suppose, ultimately, to answer your question. Yes, A lot of these apps, these social media apps, have been detrimental to mental health. But that's only parts of what they do that are that are detrimental. They also have their pros, I suppose. Um what do you What do you think about that?

SPEAKER 5
Yeah, I'm I guess I'm a generation that grew up on the fringe of having technology around us and and also, you know, technology is is the area that I work in and I, I guess I. I just fear for the fact that it turns people more inwards to a computer for help rather than necessarily like I like your idea of the social side of it if it if it can bring them more to a social solution. Um, depending on what the mental illness is, I guess, but, um, they yeah, getting them more connected is better than not.

SPEAKER 1
That is a great point. I. I was thinking just another thing to answer that then is like remember how I was making that other point about like, you sh You should only have the apps where people only need it as it's needed, right? And then get them off the site, right? Like facilitate a meet up using the web and then meet up off the web. Right. So it's that type of thing because I, I My my opinion is that a lot of this whole mental health crisis is largely socios structural and things. And if we, you know, can improve social structures and have more, you know, um, can address. You know, E inequities in all these issues and that that would be good. Um, and I am a kind of a little concerned about maybe the the over a application as a some type of silver bullet for all of this stuff. Yeah.

SPEAKER 6
All right. Lisa. Uh, yeah, I I'd like to ask, uh, you know, in psychotherapy, there is a lot of personal private information. So, uh, how do these a I tools prevent information leakage if we use, uh, a I tools to treat mental health problem?

SPEAKER 1
Sure. Um, this ultimately comes down to Where is the data going to? Um, if it's all locally, I suppose you could imagine the safest would be if you know, a hospital. You know, clinic keeps all the data in their own IT premises. Um, that might not be possible. If you're using, like, a large language models. Unless the hospital trains their own large language model, which is unlikely, then it's gonna have to go somewhere on the cloud. In that case, um, you just have to make it as secure as possible. Obviously, do all the right things like transmission over encrypted channels and also, you know, with the LLNS with chat A PT, you have options. Um, if you use the paid API, you can send requests with the API to, like, analyse the data. But then, um, it shouldn't be kept on on the, uh, O open a I server. That's that's their deal. They go, OK, if you pay us to use our service, we don't keep your data, we analyse the data, then we just delete it. Um, it doesn't exist anymore in our system. Or I think, like there are other cases like University of Melbourne, for example, have their own generative a i platform called Spark A I. Some of you may have heard of it. It actually uses open a I technology, but it uses Microsoft's version of that which is just stored on a Sydney server. That means it's secure, you know, in the sense that it never goes past the Australian border, that information. So I suppose it's a matter of two things ultimately having all your transmission and that as secure as possible, which is something you can control as an IT organisation or your IT department can control. And also, if you need to use some fancy tool that you can't create yourself because, you know, it takes costs hundreds of millions of dollars to to train the, you know, the cutting edge LL MS. Then just use it in a way that, like you can, you know, use it in. Um uh, use. Use a provider that is the best provider you can find in terms of security.

SPEAKER 6
Yeah. Thank you.

SPEAKER 2
Any other questions? I should point out that Simon's student is one of our tutors. So, um, Fang Fang, one of the four tutors for the subject. So if you're if you have Fang, you might be interested in her. Uh uh, whatever it's called.

SPEAKER 1
She's the one who's doing the the Digital Therapeutic alliance stuff. The talk on, uh, Wednesday confirmation. Great. All right.

SPEAKER 2
OK, so Claire is saying that she substituted chat GP T for ed discussion. Um, that that's a That's a 5050.

SPEAKER 1
That one. I'll I'll, I'll give it a pass. It's OK to ask chat GP T about json or XML. Um, well, you know one thing.

SPEAKER 2
You know, when you talk about the the degrading of the, um, communities, uh, it seems like that's almost a natural evolution of human, uh, communities, too. So, you know, when, like when I first started using python in the early two thousands, it was, like, famously, this very friendly, you know, community. And then as soon as it becomes, like the mainstream language and then you post something on, like stack overflow, there's always gonna be some snarky put down. Yeah, someone that's on there is like, how could you be so stupid as this question? Because that, you know, and I'm so brilliant. Um, so, yeah, it's It's a clear trade off between kind of the vulnerability of exposing yourself, um, to to other humans for things.

SPEAKER 1
So the more popular something that becomes the more of the more of it there is, the more human nature is revealed.

SPEAKER 2
Yeah, yeah, yeah. Um, anything else, or should we just wrap it up for tonight? Happy to wrap it up?

SPEAKER 1
That's that's a good good ending. Thanks for your questions. And thanks for your participation. All right.

SPEAKER 2
Thank you all. And see you on Wednesday for health.

SPEAKER 1
OK, bye. Thank you.

SPEAKER 5
Thank
